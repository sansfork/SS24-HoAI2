{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Oleg Bushtyrkov | k12338089 | 17.06.2024, 09:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 6 – Language Modeling with LSTM (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> N. Rekabsaz, B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, M. Abbass, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 10-06-2024\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u6_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u6_utils.py</code> need to be installed.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipdb in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (0.13.13)\n",
      "Requirement already satisfied: tomli in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipdb) (2.0.1)\n",
      "Requirement already satisfied: decorator in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipdb) (5.1.1)\n",
      "Requirement already satisfied: ipython>=7.31.1 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipdb) (8.16.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.19.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (1.1.3)\n",
      "Requirement already satisfied: backcall in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (5.10.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (3.0.39)\n",
      "Requirement already satisfied: pickleshare in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.6.3)\n",
      "Requirement already satisfied: matplotlib-inline in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.1.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.31.1->ipdb) (0.2.8)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olegbushtyrkov/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.10 (✓)\n",
      "Installed numpy version: 1.26.4 (✓)\n",
      "Installed pandas version: 2.2.1 (✓)\n",
      "Installed PyTorch version: 2.1.1+cu121 (✓)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb\n",
    "import u6_utils as u6\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set_theme()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u6.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u6.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Training and Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Setup the data set using the same parameter settings as in the main exercise notebook but with the changes mentioned below.</li>\n",
    "            <li>Change the batch size in the initial parameters to $64$ and observe its effect on the created batches. Explain how the corpora are transformed into batches.</li>\n",
    "            <li>Use a seed of $23$.</li>\n",
    "            <li>For a specific sequence in <code>val_data_splits</code> (e.g., index $15$), print the corresponding words of its first 25 wordIDs.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "video\n",
      "of\n",
      "in\n",
      "and\n",
      "as\n",
      "accounts\n",
      "crude\n",
      "marine\n",
      "it\n",
      "vested\n",
      "on\n",
      "<eos>\n",
      "'re\n",
      "said\n",
      "of\n",
      "is\n",
      "the\n",
      "looked\n",
      "sales\n",
      "<unk>\n",
      "a\n",
      "as\n",
      "<eos>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# Input & output parameters\n",
    "data_path = os.path.join(\"resources\", \"penn\")\n",
    "save_path = \"model.pt\" # path to save the final model\n",
    "\n",
    "# Training & evaluation parameters\n",
    "train_batch_size = 64 # batch size for training\n",
    "eval_batch_size = 64 # batch size for validation/test\n",
    "max_seq_len = 40 # sequence length\n",
    "\n",
    "# Random seed to facilitate reproducibility\n",
    "torch.manual_seed(23)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_corpus = u6.Corpus(os.path.join(data_path, \"train.txt\"))\n",
    "valid_corpus = u6.Corpus(os.path.join(data_path, \"valid.txt\"))\n",
    "test_corpus = u6.Corpus(os.path.join(data_path, \"test.txt\"))\n",
    "\n",
    "dictionary = u6.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "\n",
    "train_data_splits = u6.batchify(train_data, train_batch_size, device)\n",
    "val_data_splits = u6.batchify(valid_data, eval_batch_size, device)\n",
    "test_data_splits = u6.batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "for i in range(25):\n",
    "    print(dictionary.idx2word[int(val_data_splits[15][i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpora are iterated through, and 'batch-size'-many slices are made throughout each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Copy the implementation of <code>LM_LSTMModel</code> from the main exercise notebook but make the following changes:</li>\n",
    "            <ul>\n",
    "                <li>Add an integer parameter to <code>LM_LSTMModel</code>'s initialization, called <code>num_layers</code> which indicates the number of (vertically) stacked LSTM blocks. Hint: PyTorch's LSTM implementation directly supports this, so you simply have to set it when creating the LSTM instance (see parameter <code>num_layers</code> in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">documentation</a>).</li>\n",
    "                <li>Add a new bool parameter to <code>LM_LSTMModel</code>'s initialization, called <code>tie_weights</code>. Extend the implementation of <code>LM_LSTMModel</code> such that if <code>tie_weights</code> is set to <code>True</code>, the model ties/shares the parameters of <code>encoder</code> with the ones of <code>decoder</code>. Consider that <code>encoder</code> and <code>decoder</code> still remain separate components but their parameters are now the same (shared). This process is called <i>weight tying</i>. Feel free to search the internet for relevant resources and implementation hints.</li>\n",
    "            </ul>\n",
    "            <li>Create four models:</li>\n",
    "            <ul>\n",
    "                <li>1 layer and without weight tying</li>\n",
    "                <li>1 layer and with weight tying</li>\n",
    "                <li>2 layers and without weight tying</li>\n",
    "                <li>2 layers and with weight tying</li>\n",
    "            </ul>\n",
    "            <li>Compare the number of parameters of the models and report your observations.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 4332001\n",
      "\n",
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 2331801\n",
      "\n",
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 4653601\n",
      "\n",
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 2653401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "class LM_LSTMModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhid, num_layers, tie_weights):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp) # matrix E in the figure\n",
    "        self.rnn = torch.nn.LSTM(ninp, nhid, num_layers)\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken) # matrix U in the figure\n",
    "\n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "    \n",
    "    def forward(self, input, hidden=None, return_logs=True):\n",
    "        #ipdb.set_trace()\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        if return_logs:\n",
    "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
    "        else:\n",
    "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
    "        \n",
    "        return y_hat, last_hidden\n",
    "        \n",
    "# Model parameters\n",
    "emsize = 200  # size of word embeddings\n",
    "nhid = 200  # number of hidden units per layer\n",
    "\n",
    "model1 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=1, tie_weights=False)\n",
    "model2 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=1, tie_weights=True)\n",
    "model3 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=2, tie_weights=False)\n",
    "model4 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=2, tie_weights=True)\n",
    "model_list = [model1, model2, model3, model4]\n",
    "for model in model_list:\n",
    "    print(f\"{model}\")\n",
    "    print(f\"Model total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models with 2 layers had more trainable parameters.  \n",
    "Other than that there is little difference between the model pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3. [30 Points]</b>\n",
    "    <ul>\n",
    "        <li>Using the same setup as in the main lecture/exercise notebook, train all four models for $5$ epochs.</li>\n",
    "        <li>Using <code>ipdb</code>, look inside the <code>forward</code> function of <code>LM_LSTMModel</code> during training. Check the forward process from input to output particularly by looking at the shapes of tensors. Report the shape of all tensors used in <code>forward</code>. Try to translate the numbers into batches $B$ and sequence length $L$. For instance, if we know that the batch size is $B=32$, a tensor of shape $(32, 128, 3)$ can be interpreted as a batch of $32$ sequences with $3$ channels of size $L=128$. Thus, this tensor can be translated into $(32, 128, 3) \\rightarrow (B, L, 3)$. Look at the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">official documentation</a> to understand the order of the dimensions.</li>\n",
    "        <li>Evaluate the models. Compare the performances of all four models on the train, validation and test set (for the test set, use the best model according to the respective validation set performance), and report your observations. To do so, create a plot showing the following curves:</li>\n",
    "        <ul>\n",
    "            <li>Loss on each current training batch before every model update step as function of epochs</li>\n",
    "            <li>Loss on the validation set at every epoch</li>\n",
    "        </ul>\n",
    "        <li>Comment on the results!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 491.55 | loss  9.59 | perplexity 14556.09\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 449.78 | loss  9.22 | perplexity 10053.45\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 436.24 | loss  9.22 | perplexity 10065.21\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 445.06 | loss  9.22 | perplexity 10060.87\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 437.10 | loss  9.22 | perplexity 10062.39\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 437.19 | loss  9.22 | perplexity 10060.10\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 437.92 | loss  9.22 | perplexity 10060.75\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 437.19 | loss  9.22 | perplexity 10060.88\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 435.02 | loss  9.22 | perplexity 10060.06\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 435.44 | loss  9.22 | perplexity 10061.17\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 439.11 | loss  9.22 | perplexity 10058.16\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 440.15 | loss  9.22 | perplexity 10061.44\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 437.33 | loss  9.22 | perplexity 10064.70\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 435.25 | loss  9.22 | perplexity 10053.38\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 164.81s| valid loss  9.22 | valid perplexity 10062.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.22 | test perplexity 10058.65\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 452.08 | loss  9.59 | perplexity 14556.09\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 440.74 | loss  9.22 | perplexity 10053.45\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 433.99 | loss  9.22 | perplexity 10065.21\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 437.78 | loss  9.22 | perplexity 10060.87\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 437.65 | loss  9.22 | perplexity 10062.39\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 440.71 | loss  9.22 | perplexity 10060.10\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 442.33 | loss  9.22 | perplexity 10060.75\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 442.16 | loss  9.22 | perplexity 10060.88\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 438.18 | loss  9.22 | perplexity 10060.06\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 439.34 | loss  9.22 | perplexity 10061.17\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 439.90 | loss  9.22 | perplexity 10058.16\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 439.49 | loss  9.22 | perplexity 10061.44\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 438.58 | loss  9.22 | perplexity 10064.70\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 445.43 | loss  9.22 | perplexity 10053.38\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 164.16s| valid loss  9.22 | valid perplexity 10062.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.22 | test perplexity 10058.65\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 447.84 | loss  9.59 | perplexity 14556.09\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 432.32 | loss  9.22 | perplexity 10053.45\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 429.96 | loss  9.22 | perplexity 10065.21\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 430.40 | loss  9.22 | perplexity 10060.87\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 431.51 | loss  9.22 | perplexity 10062.39\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 431.38 | loss  9.22 | perplexity 10060.10\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 430.65 | loss  9.22 | perplexity 10060.75\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 432.67 | loss  9.22 | perplexity 10060.88\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 433.45 | loss  9.22 | perplexity 10060.06\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 434.31 | loss  9.22 | perplexity 10061.17\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 432.05 | loss  9.22 | perplexity 10058.16\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 432.56 | loss  9.22 | perplexity 10061.44\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 432.19 | loss  9.22 | perplexity 10064.70\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 433.21 | loss  9.22 | perplexity 10053.38\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 161.34s| valid loss  9.22 | valid perplexity 10062.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.22 | test perplexity 10058.65\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 447.84 | loss  9.59 | perplexity 14556.09\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 430.39 | loss  9.22 | perplexity 10053.45\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 430.37 | loss  9.22 | perplexity 10065.21\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 432.63 | loss  9.22 | perplexity 10060.87\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 481.56 | loss  9.22 | perplexity 10062.39\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 443.45 | loss  9.22 | perplexity 10060.10\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 444.56 | loss  9.22 | perplexity 10060.75\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 446.23 | loss  9.22 | perplexity 10060.88\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 445.59 | loss  9.22 | perplexity 10060.06\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 462.56 | loss  9.22 | perplexity 10061.17\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 446.94 | loss  9.22 | perplexity 10058.16\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 449.14 | loss  9.22 | perplexity 10061.44\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 438.72 | loss  9.22 | perplexity 10064.70\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 438.64 | loss  9.22 | perplexity 10053.38\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 165.75s| valid loss  9.22 | valid perplexity 10062.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.22 | test perplexity 10058.65\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 449.07 | loss  9.59 | perplexity 14556.09\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 433.24 | loss  9.22 | perplexity 10053.45\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 434.93 | loss  9.22 | perplexity 10065.21\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 434.35 | loss  9.22 | perplexity 10060.87\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 436.00 | loss  9.22 | perplexity 10062.39\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 435.01 | loss  9.22 | perplexity 10060.10\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 434.97 | loss  9.22 | perplexity 10060.75\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 435.76 | loss  9.22 | perplexity 10060.88\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 436.70 | loss  9.22 | perplexity 10060.06\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 436.34 | loss  9.22 | perplexity 10061.17\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 437.70 | loss  9.22 | perplexity 10058.16\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 437.22 | loss  9.22 | perplexity 10061.44\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 437.66 | loss  9.22 | perplexity 10064.70\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 437.63 | loss  9.22 | perplexity 10053.38\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 162.69s| valid loss  9.22 | valid perplexity 10062.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.22 | test perplexity 10058.65\n",
      "====================================================================================================\n",
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 448.56 | loss 11.68 | perplexity 118390.67\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 432.31 | loss 11.23 | perplexity 75417.73\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 434.45 | loss 11.24 | perplexity 75742.41\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 431.78 | loss 11.22 | perplexity 74282.86\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 432.08 | loss 11.24 | perplexity 76325.84\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 435.98 | loss 11.26 | perplexity 77704.69\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 431.82 | loss 11.23 | perplexity 75499.66\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 433.04 | loss 11.23 | perplexity 75305.12\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 447.56 | loss 11.23 | perplexity 75419.14\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 436.29 | loss 11.22 | perplexity 74919.62\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 435.86 | loss 11.22 | perplexity 74620.00\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 437.02 | loss 11.23 | perplexity 75434.57\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 434.95 | loss 11.23 | perplexity 75416.07\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 435.48 | loss 11.24 | perplexity 75970.84\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 162.47s| valid loss 11.22 | valid perplexity 74475.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.22 | test perplexity 74922.95\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 452.52 | loss 11.68 | perplexity 118390.67\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 436.21 | loss 11.23 | perplexity 75417.73\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 436.18 | loss 11.24 | perplexity 75742.41\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 436.35 | loss 11.22 | perplexity 74282.86\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 437.50 | loss 11.24 | perplexity 76325.84\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 436.63 | loss 11.26 | perplexity 77704.69\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 435.98 | loss 11.23 | perplexity 75499.66\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 436.03 | loss 11.23 | perplexity 75305.12\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 438.97 | loss 11.23 | perplexity 75419.14\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 436.15 | loss 11.22 | perplexity 74919.62\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 436.05 | loss 11.22 | perplexity 74620.00\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 435.95 | loss 11.23 | perplexity 75434.57\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 437.90 | loss 11.23 | perplexity 75416.07\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 436.43 | loss 11.24 | perplexity 75970.84\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 163.00s| valid loss 11.22 | valid perplexity 74475.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.22 | test perplexity 74922.95\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 451.71 | loss 11.68 | perplexity 118390.67\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 434.44 | loss 11.23 | perplexity 75417.73\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 433.38 | loss 11.24 | perplexity 75742.41\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 434.36 | loss 11.22 | perplexity 74282.86\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 435.18 | loss 11.24 | perplexity 76325.84\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 433.55 | loss 11.26 | perplexity 77704.69\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 434.50 | loss 11.23 | perplexity 75499.66\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 520.43 | loss 11.23 | perplexity 75305.12\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 477.04 | loss 11.23 | perplexity 75419.14\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 455.25 | loss 11.22 | perplexity 74919.62\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 434.29 | loss 11.22 | perplexity 74620.00\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 435.02 | loss 11.23 | perplexity 75434.57\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 435.53 | loss 11.23 | perplexity 75416.07\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 436.35 | loss 11.24 | perplexity 75970.84\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 166.05s| valid loss 11.22 | valid perplexity 74475.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.22 | test perplexity 74922.95\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 451.57 | loss 11.68 | perplexity 118390.67\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 435.01 | loss 11.23 | perplexity 75417.73\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 439.60 | loss 11.24 | perplexity 75742.41\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 434.47 | loss 11.22 | perplexity 74282.86\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 434.90 | loss 11.24 | perplexity 76325.84\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 438.08 | loss 11.26 | perplexity 77704.69\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 433.28 | loss 11.23 | perplexity 75499.66\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 434.24 | loss 11.23 | perplexity 75305.12\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 435.19 | loss 11.23 | perplexity 75419.14\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 433.55 | loss 11.22 | perplexity 74919.62\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 434.78 | loss 11.22 | perplexity 74620.00\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 438.38 | loss 11.23 | perplexity 75434.57\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 433.51 | loss 11.23 | perplexity 75416.07\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 433.57 | loss 11.24 | perplexity 75970.84\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 162.47s| valid loss 11.22 | valid perplexity 74475.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.22 | test perplexity 74922.95\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 450.88 | loss 11.68 | perplexity 118390.67\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 434.46 | loss 11.23 | perplexity 75417.73\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 436.64 | loss 11.24 | perplexity 75742.41\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 435.41 | loss 11.22 | perplexity 74282.86\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 433.88 | loss 11.24 | perplexity 76325.84\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 433.28 | loss 11.26 | perplexity 77704.69\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 456.06 | loss 11.23 | perplexity 75499.66\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 443.11 | loss 11.23 | perplexity 75305.12\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 436.27 | loss 11.23 | perplexity 75419.14\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 435.38 | loss 11.22 | perplexity 74919.62\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 442.82 | loss 11.22 | perplexity 74620.00\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 437.50 | loss 11.23 | perplexity 75434.57\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 437.65 | loss 11.23 | perplexity 75416.07\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 437.82 | loss 11.24 | perplexity 75970.84\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 163.61s| valid loss 11.22 | valid perplexity 74475.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.22 | test perplexity 74922.95\n",
      "====================================================================================================\n",
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 511.64 | loss  9.58 | perplexity 14450.62\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 492.85 | loss  9.21 | perplexity  9997.54\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 493.34 | loss  9.21 | perplexity  9999.77\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 502.46 | loss  9.21 | perplexity  9999.32\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 491.59 | loss  9.21 | perplexity  9999.14\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 491.19 | loss  9.21 | perplexity 10008.05\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 491.99 | loss  9.21 | perplexity 10000.96\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 492.26 | loss  9.21 | perplexity  9996.92\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 492.98 | loss  9.21 | perplexity  9998.51\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 492.53 | loss  9.21 | perplexity  9995.40\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 493.35 | loss  9.21 | perplexity  9993.21\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 493.67 | loss  9.21 | perplexity  9996.64\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 491.92 | loss  9.21 | perplexity  9997.05\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 493.61 | loss  9.21 | perplexity  9996.41\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 184.00s| valid loss  9.21 | valid perplexity  9995.76\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.21 | test perplexity 9997.45\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 504.59 | loss  9.58 | perplexity 14450.62\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 486.45 | loss  9.21 | perplexity  9997.54\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 486.51 | loss  9.21 | perplexity  9999.77\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 485.76 | loss  9.21 | perplexity  9999.32\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 485.73 | loss  9.21 | perplexity  9999.14\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 485.80 | loss  9.21 | perplexity 10008.05\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 484.43 | loss  9.21 | perplexity 10000.96\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 484.79 | loss  9.21 | perplexity  9996.92\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 485.64 | loss  9.21 | perplexity  9998.51\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 485.84 | loss  9.21 | perplexity  9995.40\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 486.67 | loss  9.21 | perplexity  9993.21\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 486.44 | loss  9.21 | perplexity  9996.64\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 487.37 | loss  9.21 | perplexity  9997.05\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 491.42 | loss  9.21 | perplexity  9996.41\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 181.36s| valid loss  9.21 | valid perplexity  9995.76\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.21 | test perplexity 9997.45\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 504.73 | loss  9.58 | perplexity 14450.62\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 484.96 | loss  9.21 | perplexity  9997.54\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 485.79 | loss  9.21 | perplexity  9999.77\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 485.73 | loss  9.21 | perplexity  9999.32\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 488.72 | loss  9.21 | perplexity  9999.14\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 486.14 | loss  9.21 | perplexity 10008.05\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 487.28 | loss  9.21 | perplexity 10000.96\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 486.78 | loss  9.21 | perplexity  9996.92\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 487.88 | loss  9.21 | perplexity  9998.51\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 487.13 | loss  9.21 | perplexity  9995.40\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 487.58 | loss  9.21 | perplexity  9993.21\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 501.01 | loss  9.21 | perplexity  9996.64\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 488.92 | loss  9.21 | perplexity  9997.05\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 489.93 | loss  9.21 | perplexity  9996.41\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 182.12s| valid loss  9.21 | valid perplexity  9995.76\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.21 | test perplexity 9997.45\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 504.61 | loss  9.58 | perplexity 14450.62\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 485.80 | loss  9.21 | perplexity  9997.54\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 485.52 | loss  9.21 | perplexity  9999.77\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 487.65 | loss  9.21 | perplexity  9999.32\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 485.77 | loss  9.21 | perplexity  9999.14\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 485.25 | loss  9.21 | perplexity 10008.05\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 485.99 | loss  9.21 | perplexity 10000.96\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 486.79 | loss  9.21 | perplexity  9996.92\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 486.69 | loss  9.21 | perplexity  9998.51\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 487.70 | loss  9.21 | perplexity  9995.40\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 491.51 | loss  9.21 | perplexity  9993.21\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 486.90 | loss  9.21 | perplexity  9996.64\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 487.16 | loss  9.21 | perplexity  9997.05\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 487.60 | loss  9.21 | perplexity  9996.41\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 181.55s| valid loss  9.21 | valid perplexity  9995.76\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.21 | test perplexity 9997.45\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 503.37 | loss  9.58 | perplexity 14450.62\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 483.94 | loss  9.21 | perplexity  9997.54\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 484.42 | loss  9.21 | perplexity  9999.77\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 484.74 | loss  9.21 | perplexity  9999.32\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 99152.18 | loss  9.21 | perplexity  9999.14\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 512.62 | loss  9.21 | perplexity 10008.05\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 510.20 | loss  9.21 | perplexity 10000.96\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 531.54 | loss  9.21 | perplexity  9996.92\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 551.83 | loss  9.21 | perplexity  9998.51\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 530.66 | loss  9.21 | perplexity  9995.40\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 540.40 | loss  9.21 | perplexity  9993.21\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 541.85 | loss  9.21 | perplexity  9996.64\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 520.81 | loss  9.21 | perplexity  9997.05\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 487.68 | loss  9.21 | perplexity  9996.41\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 2656.62s| valid loss  9.21 | valid perplexity  9995.76\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.21 | test perplexity 9997.45\n",
      "====================================================================================================\n",
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 504.90 | loss  9.78 | perplexity 17635.97\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 483.63 | loss  9.40 | perplexity 12122.13\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 483.39 | loss  9.40 | perplexity 12061.65\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 483.20 | loss  9.40 | perplexity 12060.15\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 484.74 | loss  9.40 | perplexity 12096.53\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 485.89 | loss  9.40 | perplexity 12140.85\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 485.23 | loss  9.41 | perplexity 12173.50\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 487.74 | loss  9.40 | perplexity 12064.82\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 485.61 | loss  9.40 | perplexity 12073.43\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 484.28 | loss  9.40 | perplexity 12140.90\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 488.02 | loss  9.40 | perplexity 12133.39\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 483.37 | loss  9.40 | perplexity 12069.15\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 507.28 | loss  9.40 | perplexity 12115.87\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 487.50 | loss  9.40 | perplexity 12087.85\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 181.58s| valid loss  9.41 | valid perplexity 12154.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12068.48\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 498.27 | loss  9.78 | perplexity 17635.97\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 480.35 | loss  9.40 | perplexity 12122.13\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 479.62 | loss  9.40 | perplexity 12061.65\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 487.13 | loss  9.40 | perplexity 12060.15\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 519.87 | loss  9.40 | perplexity 12096.53\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 492.10 | loss  9.40 | perplexity 12140.85\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 496.69 | loss  9.41 | perplexity 12173.50\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 487.59 | loss  9.40 | perplexity 12064.82\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 486.28 | loss  9.40 | perplexity 12073.43\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 486.99 | loss  9.40 | perplexity 12140.90\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 487.45 | loss  9.40 | perplexity 12133.39\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 487.81 | loss  9.40 | perplexity 12069.15\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 486.86 | loss  9.40 | perplexity 12115.87\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 486.83 | loss  9.40 | perplexity 12087.85\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 182.34s| valid loss  9.41 | valid perplexity 12154.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12068.48\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 506.55 | loss  9.78 | perplexity 17635.97\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 487.82 | loss  9.40 | perplexity 12122.13\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 488.60 | loss  9.40 | perplexity 12061.65\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 487.67 | loss  9.40 | perplexity 12060.15\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 487.68 | loss  9.40 | perplexity 12096.53\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 189596.38 | loss  9.40 | perplexity 12140.85\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 547.08 | loss  9.41 | perplexity 12173.50\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 499.08 | loss  9.40 | perplexity 12064.82\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 498.40 | loss  9.40 | perplexity 12073.43\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 497.50 | loss  9.40 | perplexity 12140.90\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 490.77 | loss  9.40 | perplexity 12133.39\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 489.98 | loss  9.40 | perplexity 12069.15\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 490.44 | loss  9.40 | perplexity 12115.87\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 493.73 | loss  9.40 | perplexity 12087.85\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 4913.85s| valid loss  9.41 | valid perplexity 12154.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12068.48\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 587.87 | loss  9.78 | perplexity 17635.97\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 529.07 | loss  9.40 | perplexity 12122.13\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 521.51 | loss  9.40 | perplexity 12061.65\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 538.82 | loss  9.40 | perplexity 12060.15\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 536.69 | loss  9.40 | perplexity 12096.53\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 506.23 | loss  9.40 | perplexity 12140.85\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 510.62 | loss  9.41 | perplexity 12173.50\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 516.45 | loss  9.40 | perplexity 12064.82\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 573.97 | loss  9.40 | perplexity 12073.43\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 550.54 | loss  9.40 | perplexity 12140.90\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 536.61 | loss  9.40 | perplexity 12133.39\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 568.89 | loss  9.40 | perplexity 12069.15\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 622.69 | loss  9.40 | perplexity 12115.87\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 621.90 | loss  9.40 | perplexity 12087.85\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 204.32s| valid loss  9.41 | valid perplexity 12154.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12068.48\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 521.48 | loss  9.78 | perplexity 17635.97\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 551.52 | loss  9.40 | perplexity 12122.13\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 509.07 | loss  9.40 | perplexity 12061.65\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 533.39 | loss  9.40 | perplexity 12060.15\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 515.90 | loss  9.40 | perplexity 12096.53\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 502.26 | loss  9.40 | perplexity 12140.85\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 517.65 | loss  9.41 | perplexity 12173.50\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 521.05 | loss  9.40 | perplexity 12064.82\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 527.36 | loss  9.40 | perplexity 12073.43\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 537.25 | loss  9.40 | perplexity 12140.90\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 514.11 | loss  9.40 | perplexity 12133.39\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 504.83 | loss  9.40 | perplexity 12069.15\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 502.63 | loss  9.40 | perplexity 12115.87\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 521.80 | loss  9.40 | perplexity 12087.85\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 195.14s| valid loss  9.41 | valid perplexity 12154.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12068.48\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "loss_train = []\n",
    "\n",
    "CUT_AFTER_BATCHES = -1  # JUST FOR DEBUGGING: cut the loop after these number of batches. Set to -1 to ignore\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dictionary: u6.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, train_data_splits,\n",
    "          clipping: float, learning_rate: float, print_interval: int, epoch: int,\n",
    "          criterion: torch.nn.Module = torch.nn.NLLLoss()):\n",
    "    \"\"\"\n",
    "    Train the model. Training mode turned on to enable dropout.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (train_data_splits.size(0) - 1) // max_seq_len\n",
    "\n",
    "    loss_collector = []\n",
    "    \n",
    "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
    "        batch_data, batch_targets = u6.get_batch(train_data_splits, i, max_seq_len)\n",
    "        # ipdb.set_trace()\n",
    "        \n",
    "        # Don't forget it! Otherwise, the gradients are summed together!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Repackaging batches only keeps the value of start_hidden and disconnects its computational graph.\n",
    "        # If repackaging is not done the, gradients are calculated from the current point to the beginning\n",
    "        # of the sequence which becomes computationally too expensive.\n",
    "        if start_hidden is not None:\n",
    "            start_hidden = u6.repackage_hidden(start_hidden)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "        \n",
    "        # Loss computation & backward pass\n",
    "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "        loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # The last hidden states of the current step is set as the start hidden state of the next step.\n",
    "        # This passes the information of the current batch to the next batch.\n",
    "        start_hidden = last_hidden\n",
    "        \n",
    "        # Clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        \n",
    "        # Updating parameters using SGD\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_i % print_interval == 0 and batch_i > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            loss_collector.append(cur_loss)\n",
    "            elapsed = time.time() - start_time\n",
    "            throughput = elapsed * 1000 / print_interval\n",
    "            print(f\"| epoch {epoch:3d} | {batch_i:5d}/{n_batches:5d} batches | lr {learning_rate:02.2f} | ms/batch {throughput:5.2f} \"\n",
    "                  f\"| loss {cur_loss:5.2f} | perplexity {math.exp(cur_loss):8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    loss_train.append(loss_collector)\n",
    "            \n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "loss_val = []\n",
    "\n",
    "\n",
    "best_val_loss = None\n",
    "model = model1\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "model = model2\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "model = model3\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "model = model4\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7967b81be3b0>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81be440>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81be2f0>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bdba0>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bfc70>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bece0>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bf2e0>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bf340>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bdf00>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81be530>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bfa60>,\n",
       " <matplotlib.lines.Line2D at 0x7967b81bc820>,\n",
       " <matplotlib.lines.Line2D at 0x7967b7e8ac50>,\n",
       " <matplotlib.lines.Line2D at 0x7967b7e8b490>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train, loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "Model 2 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "Model 3 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "Model 4 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "  \n",
    "The plot turned out to be linear...  \n",
    "Since I assume this was not the intended result. It seems there was a problem with either my model implementations, or the transferring of code across notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Generation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4. [30 Points]</b>\n",
    "    <p>\n",
    "    Copy the language generation code from the main exercise notebook and perform the following tasks:\n",
    "    </p>\n",
    "        <ul>\n",
    "            <li>Compare all four previous models by generating $12$ words that append the starting word <tt>\"despite\"</tt>.</li>\n",
    "            <li>For each model, retrieve the top $10$ wordIDs with the highest probabilities from the generated probability distribution (<code>prob_dist</code>) following the starting word <tt>\"despite\"</tt>. Fetch the corresponding words of these wordIDs. Do you observe any specific linguistic characteristic common between these words?</li>\n",
    "            <li>The implementation in the main exercise notebook is based on sampling. Implement a second deterministic variant based on the <i>top-1</i> approach. In this particular variant, the generated word is the word with the highest probability in the predicted probability distribution. Repeat the same procedure as before (i.e., generate $12$ words that append the starting word <tt>\"despite\"</tt>).</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "['americans', 'warren', 'underwrite', 'enough', 'neutrons', 'weisfield', 'khmer', 'angeles-based', 'palladium', 'minds']\n",
      "['employ', 'principals', 'instructed', 'pipeline', 'length', 'gm', 'foley', 'sanctions', 'rubicam', 'republic']\n",
      "['ig', 'doomed', 'hardly', 'turmoil', 'bougainville', 'situations', 'kick', 'oddly', 'orders', 'edt']\n",
      "['pilot', 'offsetting', 'backdrop', 'pearce', 'rumor', 'door', 'n.v', 'bankruptcy-court', 'difficulty', 'ncnb']\n",
      "['lackluster', 'predictably', 'deteriorating', 'contribute', 'crushed', 'limit', 'gasoline', 'precious', 'embarrassment', 'corp.']\n",
      "['resigned', 'situations', 'humor', 'coverage', 'commercial', 'survival', 'collins', 'surprise', 'convention', 'circumstances']\n",
      "['roadway', 'photo', 'zone', 'shape', 'thereafter', 'tide', 'troubles', 'back', 'records', 'mario']\n",
      "['shoppers', 'starts', 'calm', 'corruption', 'analytical', 'westmoreland', 'doubtful', 'pa.', 'slowed', 'charlie']\n",
      "['banque', 'kicked', 'lotus', 'murdoch', 'recreation', 'media', 'foreclosed', 'nights', 'weighed', 'equity-purchase']\n",
      "['conservation', 'nsc', 'warburg', 'kodak', 'cease-fire', 'zones', 'newsweek', 'square', 'privatization', 'begins']\n",
      "['price-earnings', 'middlemen', 'industrial', 'dissident', 'clinton', 'reflection', 'orderly', 'kid', \"'re\", 'swapo']\n",
      "['precious', 'dalkon', 'cutting', 'electronic', 'underestimated', 'lenses', 'cosmetic', 'citizens', 'pilson', 'panamanian']\n",
      "despite bipartisan warns touchy environmentalists downgrading family fronts oils eddie spoken obviously atlanta\n",
      "Model 1\n",
      "['responded', 'anniversary', 'draft', 'toledo', 'pet', 'learn', 'racing', 'stein', 'preamble', 'winnebago']\n",
      "['meeting', 'ailing', 'reaching', 'curtail', 'friendship', 'g.m.b', 'favors', 'jaguar', 'oral', 'spreading']\n",
      "['steinberg', 'catastrophes', 'six', 'circles', 'texaco', 'buildup', 'magic', 'depended', 'amicable', 'chaos']\n",
      "['capability', 'prepaid', 'knew', 'races', 'rake', 'gotten', 'strategic', 'built', 'appliances', 'rebels']\n",
      "['computerized', 'reagan', 'publishing', 'atmospheric', 'cheating', 'additions', 'delmed', 'nec', 'reserves', 'decades']\n",
      "['cancer', 'departments', 'blanket', 'assembly', 'enjoying', 'disproportionate', 'apartheid', 'hits', 'anniversary', 'dubious']\n",
      "['winter', 'exports', 'hours', 'standard', 'china', 'types', 'fried', 'premier', 'sidelines', 'mcdonough']\n",
      "['sure', 'minorities', 'basis', 'ore.', 'continent', 'recycling', 'accepting', 'mortgages', 'disk-drive', 'tower']\n",
      "['showroom', 'actor', 'moved', 'glossy', 'schedules', 'luck', 'eroded', 'cheney', 'shall', 'toyota']\n",
      "['throw', 'carlos', 'atmospheric', 'purposes', 'skipper', 'creditor', 'particularly', 'creditor', 'hartford', 'redemption']\n",
      "['hectic', 'ethical', 'bronner', 'espn', 'physics', 'again', 'joel', 'streets', 'bronner', 'fitzwater']\n",
      "['benson', 'busiest', 'temple', 'chores', 'fool', 'ride', 'dun', 'automotive', 'warned', 'loans']\n",
      "despite ozone fend analyst kennedy proposes essential undercut executing advise canadian golden scenes\n",
      "Model 2\n",
      "['chile', 'distribution', 'probably', 'hampshire', 'house-passed', 'eighth', 'packaging', 'similar', 'prepares', 'off']\n",
      "['recognize', 'pushed', 'border', 'chefs', 'weiss', 'redeem', 'critic', 'pump', 'divisive', '#']\n",
      "['text', 'creatures', 'restructuring', 'daiwa', 'featured', 'uncovered', 'noriega', 'miss', 'europe', 'chicken']\n",
      "['marlowe', 'neighboring', 'death', 'tanker', 'answers', 'taping', 'terrorist', 'colombia', 'spent', 'parties']\n",
      "['fixed-income', 'desirable', 'midland', 'valid', 'dentsu', 'remodeling', 'kicked', 'guterman', 'samples', 'conservatorship']\n",
      "['repayment', 'president', 'trends', 'gmac', 'ralston', 'my', 'was', 'intentionally', 'bet', 'justify']\n",
      "['tribune', 'question', 'deficiency', 'saturday', 'language', 'year-to-year', 'fueling', 'fell', 'lighting', 'george']\n",
      "['ceiling', 'aroused', 'citicorp', 'valid', 'peninsula', 'laying', 'steel', 'constituents', 'tool', 'microsystems']\n",
      "['mengistu', 'fundamentally', 'western', 'unspecified', 'mitsui', 'wrongdoing', 'damn', 'milton', 'judging', 'sounds']\n",
      "['bulls', 'wilfred', 'santa', 'saving', 'takeover', 'rockefeller', 'fledgling', 'banned', 'sluggish', 'kurt']\n",
      "['krenz', 'conclusion', \"o'connell\", 'cocaine', 'maximum', 'mac', 'conviction', 'contractors', 'establishing', 'daniel']\n",
      "['character', 'dropping', 'monitor', 'ignore', 'upheld', 'mergers', 't.', 'drabinsky', 'instance', 'mired']\n",
      "despite speeding <oov> loser likes patent k. monsanto instrument vietnam contingent qualified setback\n",
      "Model 3\n",
      "['dip', 'refrigerators', 'arias', 'hostile', 'insisted', 'rake', 'forge', 'williams', 'prohibits', 'prepared']\n",
      "['catastrophic', 'powers', 'cathcart', 'upheld', 'equipment', 'career', 'favors', 'loral', 'michelin', 'perhaps']\n",
      "['difference', 'instructed', 'reduce', 'rape', 'second-quarter', 'greece', 'mirror', 'old-fashioned', 'throws', 'burger']\n",
      "['defeat', 'acting', 'lists', 'george', 'control', 'standing', 'murdered', 'casting', 'preclude', 'everywhere']\n",
      "['happens', 'shipping', 'talked', 'subordinated', 'tass', 'sociologist', 'guaranteed', 'operate', 'stevens', 'interpret']\n",
      "['easy', 'wayne', 'figure', 'statutory', 'escape', 'soda', 'spirit', 'continually', 'coupled', 'haas']\n",
      "['seasonally', 'deposit', 'discounts', 'switch', 'mega-issues', 'among', 'privately', 'hat', 'diversifying', 'courtaulds']\n",
      "['deliberations', 'fiber', 'quebecor', 'chancery', 'hunt', 'stuff', 'integrity', 'dressed', 'california', 'ratios']\n",
      "['coatings', 'large-scale', 'respected', 'smoothly', 'latter', 'troubles', 'midmorning', 'provinces', 'recalls', 'contemplating']\n",
      "['pilot', 'leverage', 'again', 'fla.', 'supportive', 'english', 'individuals', 'jacob', 'somebody', 'skiing']\n",
      "['baseball', 'grants', 'form', 'neighbors', 'branches', 'ufo', 'british', 'jointly', 'voting', 'fulton']\n",
      "['senior', 'attended', 'senior', 'crumbling', 'strengthened', 'pose', 'retinoblastoma', 'ual', 'actress', 'walk']\n",
      "despite wire merger refineries punishment banponce expensive schwartz census makers willingness assist hinted\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "for model in model_list:\n",
    "    print(f\"Model {model_list.index(model)}\")\n",
    "    \n",
    "    GENERATION_LENGTH = 12\n",
    "    START_WORD = \"despite\"\n",
    "    \n",
    "    start_hidden = None\n",
    "    START_WORD = START_WORD.lower()\n",
    "    generated_text = START_WORD\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        wordid_input = dictionary.word2idx[START_WORD]\n",
    "        for i in range(0, GENERATION_LENGTH):\n",
    "            data = u6.batchify(torch.tensor([wordid_input]), 1, device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
    "            \n",
    "            prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze())\n",
    "            \n",
    "            samples = prob_dist.sample((1,10)).numpy()\n",
    "            sample_words = [dictionary.idx2word[i] for i in samples[0]]\n",
    "            print(sample_words)\n",
    "            \n",
    "            wordid_input = prob_dist.sample()\n",
    "            word_generated = dictionary.idx2word[wordid_input]\n",
    "            \n",
    "            generated_text += \" \" + word_generated\n",
    "            \n",
    "            start_hidden = last_hidden\n",
    "    \n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've noticed that most of the sampled words are either nouns, actions, or characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "despite counsel clinic kong collectors letting highly fundamentally catastrophe secure j.p. tass wear\n",
      "Model 1\n",
      "despite shannon adjust manville glass wake j.p. guardian manpower stronger bids staffs appropriate\n",
      "Model 2\n",
      "despite paper bank existence wedge norton f-14 hazard reflects hidden limited base installation\n",
      "Model 3\n",
      "despite publishers monopoly flows proposition ill. arco stunning tried bribe squeeze pork-barrel drain\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    print(f\"Model {model_list.index(model)}\")\n",
    "    \n",
    "    GENERATION_LENGTH = 12\n",
    "    START_WORD = \"despite\"\n",
    "    \n",
    "    start_hidden = None\n",
    "    START_WORD = START_WORD.lower()\n",
    "        \n",
    "    generated_text = START_WORD\n",
    "    with torch.no_grad():\n",
    "        wordid_input = dictionary.word2idx[START_WORD]\n",
    "        for i in range(0, GENERATION_LENGTH):\n",
    "            data = u6.batchify(torch.tensor([wordid_input]), 1, device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
    "            \n",
    "            prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze()) \n",
    "            wordid_input = prob_dist.sample([1])\n",
    "            word_generated = dictionary.idx2word[wordid_input]\n",
    "            \n",
    "            generated_text += \" \" + word_generated\n",
    "            \n",
    "            start_hidden = last_hidden\n",
    "    \n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
