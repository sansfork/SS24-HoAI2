{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Oleg Bushtyrkov | k12338089 | 17.06.2024, 09:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 6 – Language Modeling with LSTM (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> N. Rekabsaz, B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, M. Abbass, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 10-06-2024\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u6_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u6_utils.py</code> need to be installed.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipdb in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (0.13.13)\n",
      "Requirement already satisfied: tomli in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipdb) (2.0.1)\n",
      "Requirement already satisfied: decorator in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipdb) (5.1.1)\n",
      "Requirement already satisfied: ipython>=7.31.1 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipdb) (8.16.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n",
      "Requirement already satisfied: backcall in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.1.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (1.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (3.0.39)\n",
      "Requirement already satisfied: pickleshare in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (5.10.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.31.1->ipdb) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/olegbushtyrkov/.local/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olegbushtyrkov/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.10 (✓)\n",
      "Installed numpy version: 1.26.4 (✓)\n",
      "Installed pandas version: 2.2.1 (✓)\n",
      "Installed PyTorch version: 2.1.1+cu121 (✓)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb\n",
    "import u6_utils as u6\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set_theme()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u6.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u6.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Training and Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Setup the data set using the same parameter settings as in the main exercise notebook but with the changes mentioned below.</li>\n",
    "            <li>Change the batch size in the initial parameters to $64$ and observe its effect on the created batches. Explain how the corpora are transformed into batches.</li>\n",
    "            <li>Use a seed of $23$.</li>\n",
    "            <li>For a specific sequence in <code>val_data_splits</code> (e.g., index $15$), print the corresponding words of its first 25 wordIDs.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "video\n",
      "of\n",
      "in\n",
      "and\n",
      "as\n",
      "accounts\n",
      "crude\n",
      "marine\n",
      "it\n",
      "vested\n",
      "on\n",
      "<eos>\n",
      "'re\n",
      "said\n",
      "of\n",
      "is\n",
      "the\n",
      "looked\n",
      "sales\n",
      "<unk>\n",
      "a\n",
      "as\n",
      "<eos>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# Input & output parameters\n",
    "data_path = os.path.join(\"resources\", \"penn\")\n",
    "save_path = \"model.pt\" # path to save the final model\n",
    "\n",
    "# Training & evaluation parameters\n",
    "train_batch_size = 64 # batch size for training\n",
    "eval_batch_size = 64 # batch size for validation/test\n",
    "max_seq_len = 40 # sequence length\n",
    "\n",
    "# Random seed to facilitate reproducibility\n",
    "torch.manual_seed(23)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_corpus = u6.Corpus(os.path.join(data_path, \"train.txt\"))\n",
    "valid_corpus = u6.Corpus(os.path.join(data_path, \"valid.txt\"))\n",
    "test_corpus = u6.Corpus(os.path.join(data_path, \"test.txt\"))\n",
    "\n",
    "dictionary = u6.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "\n",
    "train_data_splits = u6.batchify(train_data, train_batch_size, device)\n",
    "val_data_splits = u6.batchify(valid_data, eval_batch_size, device)\n",
    "test_data_splits = u6.batchify(test_data, eval_batch_size, device)\n",
    "\n",
    "for i in range(25):\n",
    "    print(dictionary.idx2word[int(val_data_splits[15][i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpora are iterated through, and 'batch-size'-many slices are made throughout each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Copy the implementation of <code>LM_LSTMModel</code> from the main exercise notebook but make the following changes:</li>\n",
    "            <ul>\n",
    "                <li>Add an integer parameter to <code>LM_LSTMModel</code>'s initialization, called <code>num_layers</code> which indicates the number of (vertically) stacked LSTM blocks. Hint: PyTorch's LSTM implementation directly supports this, so you simply have to set it when creating the LSTM instance (see parameter <code>num_layers</code> in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">documentation</a>).</li>\n",
    "                <li>Add a new bool parameter to <code>LM_LSTMModel</code>'s initialization, called <code>tie_weights</code>. Extend the implementation of <code>LM_LSTMModel</code> such that if <code>tie_weights</code> is set to <code>True</code>, the model ties/shares the parameters of <code>encoder</code> with the ones of <code>decoder</code>. Consider that <code>encoder</code> and <code>decoder</code> still remain separate components but their parameters are now the same (shared). This process is called <i>weight tying</i>. Feel free to search the internet for relevant resources and implementation hints.</li>\n",
    "            </ul>\n",
    "            <li>Create four models:</li>\n",
    "            <ul>\n",
    "                <li>1 layer and without weight tying</li>\n",
    "                <li>1 layer and with weight tying</li>\n",
    "                <li>2 layers and without weight tying</li>\n",
    "                <li>2 layers and with weight tying</li>\n",
    "            </ul>\n",
    "            <li>Compare the number of parameters of the models and report your observations.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 2331801\n",
      "\n",
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 2331801\n",
      "\n",
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 2653401\n",
      "\n",
      "LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total trainable parameters: 2653401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "class LM_LSTMModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhid, num_layers, tie_weights):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp) # matrix E in the figure\n",
    "        self.rnn = torch.nn.LSTM(ninp, nhid, num_layers)\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken) # matrix U in the figure\n",
    "        self.tie = bool\n",
    "\n",
    "        if self.tie:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "    \n",
    "    def forward(self, input, hidden=None, return_logs=True):\n",
    "        #ipdb.set_trace()\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        if return_logs:\n",
    "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
    "        else:\n",
    "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
    "        \n",
    "        return y_hat, last_hidden\n",
    "        \n",
    "# Model parameters\n",
    "emsize = 200  # size of word embeddings\n",
    "nhid = 200  # number of hidden units per layer\n",
    "\n",
    "model1 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=1, tie_weights=False)\n",
    "model2 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=1, tie_weights=True)\n",
    "model3 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=2, tie_weights=False)\n",
    "model4 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=2, tie_weights=True)\n",
    "model_list = [model1, model2, model3, model4]\n",
    "for model in model_list:\n",
    "    print(f\"{model}\")\n",
    "    print(f\"Model total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models with 2 layers had more trainable parameters.  \n",
    "Other than that there is little difference between the model pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3. [30 Points]</b>\n",
    "    <ul>\n",
    "        <li>Using the same setup as in the main lecture/exercise notebook, train all four models for $5$ epochs.</li>\n",
    "        <li>Using <code>ipdb</code>, look inside the <code>forward</code> function of <code>LM_LSTMModel</code> during training. Check the forward process from input to output particularly by looking at the shapes of tensors. Report the shape of all tensors used in <code>forward</code>. Try to translate the numbers into batches $B$ and sequence length $L$. For instance, if we know that the batch size is $B=32$, a tensor of shape $(32, 128, 3)$ can be interpreted as a batch of $32$ sequences with $3$ channels of size $L=128$. Thus, this tensor can be translated into $(32, 128, 3) \\rightarrow (B, L, 3)$. Look at the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">official documentation</a> to understand the order of the dimensions.</li>\n",
    "        <li>Evaluate the models. Compare the performances of all four models on the train, validation and test set (for the test set, use the best model according to the respective validation set performance), and report your observations. To do so, create a plot showing the following curves:</li>\n",
    "        <ul>\n",
    "            <li>Loss on each current training batch before every model update step as function of epochs</li>\n",
    "            <li>Loss on the validation set at every epoch</li>\n",
    "        </ul>\n",
    "        <li>Comment on the results!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 457.53 | loss 11.47 | perplexity 95655.34\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 433.56 | loss 11.02 | perplexity 61379.99\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 432.30 | loss 11.03 | perplexity 61578.79\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 440.58 | loss 11.03 | perplexity 61598.76\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 438.00 | loss 11.02 | perplexity 60944.57\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 435.00 | loss 11.02 | perplexity 61243.45\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 433.85 | loss 11.03 | perplexity 61507.12\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 520.02 | loss 11.03 | perplexity 61816.15\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 523.75 | loss 11.03 | perplexity 61871.48\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 452.02 | loss 11.02 | perplexity 61093.81\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 458.92 | loss 11.02 | perplexity 61332.92\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 484.48 | loss 11.04 | perplexity 62558.76\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 507.66 | loss 11.04 | perplexity 62271.16\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 470.08 | loss 11.04 | perplexity 62014.17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 172.49s| valid loss 11.03 | valid perplexity 61680.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.03 | test perplexity 61674.88\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 467.40 | loss 11.47 | perplexity 95655.34\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 438.60 | loss 11.02 | perplexity 61379.99\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 435.43 | loss 11.03 | perplexity 61578.79\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 431.06 | loss 11.03 | perplexity 61598.76\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 464.50 | loss 11.02 | perplexity 60944.57\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 443.29 | loss 11.02 | perplexity 61243.45\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 429.05 | loss 11.03 | perplexity 61507.12\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 431.12 | loss 11.03 | perplexity 61816.15\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 428.45 | loss 11.03 | perplexity 61871.48\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 428.54 | loss 11.02 | perplexity 61093.81\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 431.98 | loss 11.02 | perplexity 61332.92\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 430.10 | loss 11.04 | perplexity 62558.76\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 426.72 | loss 11.04 | perplexity 62271.16\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 430.40 | loss 11.04 | perplexity 62014.17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 162.52s| valid loss 11.03 | valid perplexity 61680.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.03 | test perplexity 61674.88\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 445.20 | loss 11.47 | perplexity 95655.34\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 434.82 | loss 11.02 | perplexity 61379.99\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 429.29 | loss 11.03 | perplexity 61578.79\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 426.54 | loss 11.03 | perplexity 61598.76\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 434.88 | loss 11.02 | perplexity 60944.57\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 439.93 | loss 11.02 | perplexity 61243.45\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 428.58 | loss 11.03 | perplexity 61507.12\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 427.99 | loss 11.03 | perplexity 61816.15\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 427.57 | loss 11.03 | perplexity 61871.48\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 427.94 | loss 11.02 | perplexity 61093.81\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 427.00 | loss 11.02 | perplexity 61332.92\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 430.10 | loss 11.04 | perplexity 62558.76\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 426.21 | loss 11.04 | perplexity 62271.16\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 427.38 | loss 11.04 | perplexity 62014.17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 160.38s| valid loss 11.03 | valid perplexity 61680.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.03 | test perplexity 61674.88\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 446.42 | loss 11.47 | perplexity 95655.34\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 426.41 | loss 11.02 | perplexity 61379.99\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 427.59 | loss 11.03 | perplexity 61578.79\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 428.23 | loss 11.03 | perplexity 61598.76\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 426.94 | loss 11.02 | perplexity 60944.57\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 426.25 | loss 11.02 | perplexity 61243.45\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 427.81 | loss 11.03 | perplexity 61507.12\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 428.50 | loss 11.03 | perplexity 61816.15\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 442.09 | loss 11.03 | perplexity 61871.48\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 426.67 | loss 11.02 | perplexity 61093.81\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 426.49 | loss 11.02 | perplexity 61332.92\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 428.01 | loss 11.04 | perplexity 62558.76\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 428.77 | loss 11.04 | perplexity 62271.16\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 428.19 | loss 11.04 | perplexity 62014.17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 160.01s| valid loss 11.03 | valid perplexity 61680.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.03 | test perplexity 61674.88\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 443.66 | loss 11.47 | perplexity 95655.34\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 426.69 | loss 11.02 | perplexity 61379.99\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 433.39 | loss 11.03 | perplexity 61578.79\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 427.68 | loss 11.03 | perplexity 61598.76\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 427.52 | loss 11.02 | perplexity 60944.57\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 430.56 | loss 11.02 | perplexity 61243.45\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 427.72 | loss 11.03 | perplexity 61507.12\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 427.83 | loss 11.03 | perplexity 61816.15\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 430.89 | loss 11.03 | perplexity 61871.48\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 428.62 | loss 11.02 | perplexity 61093.81\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 427.15 | loss 11.02 | perplexity 61332.92\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 431.29 | loss 11.04 | perplexity 62558.76\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 431.82 | loss 11.04 | perplexity 62271.16\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 430.60 | loss 11.04 | perplexity 62014.17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 160.22s| valid loss 11.03 | valid perplexity 61680.93\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.03 | test perplexity 61674.88\n",
      "====================================================================================================\n",
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 444.95 | loss 11.59 | perplexity 107920.82\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 431.16 | loss 11.16 | perplexity 70117.12\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 427.68 | loss 11.14 | perplexity 68601.40\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 428.41 | loss 11.15 | perplexity 69296.31\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 429.13 | loss 11.16 | perplexity 70333.07\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 428.28 | loss 11.13 | perplexity 68340.29\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 428.34 | loss 11.15 | perplexity 69838.42\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 427.56 | loss 11.16 | perplexity 70265.62\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 428.27 | loss 11.16 | perplexity 70202.44\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 429.50 | loss 11.15 | perplexity 69779.41\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 428.74 | loss 11.17 | perplexity 70792.13\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 429.76 | loss 11.16 | perplexity 70449.19\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 429.72 | loss 11.15 | perplexity 69887.45\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 430.84 | loss 11.14 | perplexity 68931.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 160.17s| valid loss 11.13 | valid perplexity 68156.99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.14 | test perplexity 69093.64\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 445.49 | loss 11.59 | perplexity 107920.82\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 431.68 | loss 11.16 | perplexity 70117.12\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 433.26 | loss 11.14 | perplexity 68601.40\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 429.60 | loss 11.15 | perplexity 69296.31\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 429.59 | loss 11.16 | perplexity 70333.07\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 429.89 | loss 11.13 | perplexity 68340.29\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 429.10 | loss 11.15 | perplexity 69838.42\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 430.40 | loss 11.16 | perplexity 70265.62\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 430.72 | loss 11.16 | perplexity 70202.44\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 430.32 | loss 11.15 | perplexity 69779.41\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 429.88 | loss 11.17 | perplexity 70792.13\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 430.24 | loss 11.16 | perplexity 70449.19\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 435.67 | loss 11.15 | perplexity 69887.45\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 430.14 | loss 11.14 | perplexity 68931.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 160.77s| valid loss 11.13 | valid perplexity 68156.99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.14 | test perplexity 69093.64\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 446.26 | loss 11.59 | perplexity 107920.82\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 429.89 | loss 11.16 | perplexity 70117.12\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 432.53 | loss 11.14 | perplexity 68601.40\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 429.62 | loss 11.15 | perplexity 69296.31\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 428.95 | loss 11.16 | perplexity 70333.07\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 447.48 | loss 11.13 | perplexity 68340.29\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 433.11 | loss 11.15 | perplexity 69838.42\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 435.04 | loss 11.16 | perplexity 70265.62\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 438.95 | loss 11.16 | perplexity 70202.44\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 432.91 | loss 11.15 | perplexity 69779.41\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 431.89 | loss 11.17 | perplexity 70792.13\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 433.00 | loss 11.16 | perplexity 70449.19\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 434.49 | loss 11.15 | perplexity 69887.45\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 433.68 | loss 11.14 | perplexity 68931.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 161.91s| valid loss 11.13 | valid perplexity 68156.99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.14 | test perplexity 69093.64\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 450.24 | loss 11.59 | perplexity 107920.82\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 433.27 | loss 11.16 | perplexity 70117.12\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 433.38 | loss 11.14 | perplexity 68601.40\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 433.47 | loss 11.15 | perplexity 69296.31\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 444.59 | loss 11.16 | perplexity 70333.07\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 457.19 | loss 11.13 | perplexity 68340.29\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 439.46 | loss 11.15 | perplexity 69838.42\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 436.25 | loss 11.16 | perplexity 70265.62\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 456.65 | loss 11.16 | perplexity 70202.44\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 453.61 | loss 11.15 | perplexity 69779.41\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 455.39 | loss 11.17 | perplexity 70792.13\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 442.09 | loss 11.16 | perplexity 70449.19\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 433.30 | loss 11.15 | perplexity 69887.45\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 434.11 | loss 11.14 | perplexity 68931.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 164.89s| valid loss 11.13 | valid perplexity 68156.99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.14 | test perplexity 69093.64\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 450.97 | loss 11.59 | perplexity 107920.82\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 432.85 | loss 11.16 | perplexity 70117.12\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 434.89 | loss 11.14 | perplexity 68601.40\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 433.35 | loss 11.15 | perplexity 69296.31\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 433.06 | loss 11.16 | perplexity 70333.07\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 435.11 | loss 11.13 | perplexity 68340.29\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 433.75 | loss 11.15 | perplexity 69838.42\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 433.65 | loss 11.16 | perplexity 70265.62\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 437.29 | loss 11.16 | perplexity 70202.44\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 444.69 | loss 11.15 | perplexity 69779.41\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 443.22 | loss 11.17 | perplexity 70792.13\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 433.31 | loss 11.16 | perplexity 70449.19\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 432.44 | loss 11.15 | perplexity 69887.45\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 433.49 | loss 11.14 | perplexity 68931.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 162.51s| valid loss 11.13 | valid perplexity 68156.99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss 11.14 | test perplexity 69093.64\n",
      "====================================================================================================\n",
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 504.95 | loss  9.77 | perplexity 17529.88\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 486.82 | loss  9.40 | perplexity 12052.37\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 485.42 | loss  9.40 | perplexity 12036.92\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 497.21 | loss  9.39 | perplexity 11967.65\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 489.98 | loss  9.39 | perplexity 11978.14\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 486.35 | loss  9.38 | perplexity 11828.07\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 485.09 | loss  9.39 | perplexity 11977.62\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 485.49 | loss  9.39 | perplexity 12007.28\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 486.36 | loss  9.39 | perplexity 12004.11\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 506.39 | loss  9.39 | perplexity 11992.41\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 511.02 | loss  9.39 | perplexity 12017.50\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 505.20 | loss  9.39 | perplexity 12019.70\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 482.50 | loss  9.39 | perplexity 12023.41\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 482.99 | loss  9.40 | perplexity 12040.52\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 183.03s| valid loss  9.39 | valid perplexity 11912.69\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12075.26\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 501.92 | loss  9.77 | perplexity 17529.88\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 486.11 | loss  9.40 | perplexity 12052.37\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 486.02 | loss  9.40 | perplexity 12036.92\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 484.29 | loss  9.39 | perplexity 11967.65\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 483.38 | loss  9.39 | perplexity 11978.14\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 484.59 | loss  9.38 | perplexity 11828.07\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 489.63 | loss  9.39 | perplexity 11977.62\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 484.83 | loss  9.39 | perplexity 12007.28\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 495.11 | loss  9.39 | perplexity 12004.11\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 515.79 | loss  9.39 | perplexity 11992.41\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 533.55 | loss  9.39 | perplexity 12017.50\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 543.24 | loss  9.39 | perplexity 12019.70\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 535.83 | loss  9.39 | perplexity 12023.41\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 545.91 | loss  9.40 | perplexity 12040.52\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 188.12s| valid loss  9.39 | valid perplexity 11912.69\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12075.26\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 504.49 | loss  9.77 | perplexity 17529.88\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 487.17 | loss  9.40 | perplexity 12052.37\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 482.19 | loss  9.40 | perplexity 12036.92\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 482.98 | loss  9.39 | perplexity 11967.65\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 482.21 | loss  9.39 | perplexity 11978.14\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 485.19 | loss  9.38 | perplexity 11828.07\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 485.35 | loss  9.39 | perplexity 11977.62\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 483.32 | loss  9.39 | perplexity 12007.28\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 482.31 | loss  9.39 | perplexity 12004.11\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 482.91 | loss  9.39 | perplexity 11992.41\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 483.96 | loss  9.39 | perplexity 12017.50\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 482.83 | loss  9.39 | perplexity 12019.70\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 482.08 | loss  9.39 | perplexity 12023.41\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 482.38 | loss  9.40 | perplexity 12040.52\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 180.36s| valid loss  9.39 | valid perplexity 11912.69\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12075.26\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 499.67 | loss  9.77 | perplexity 17529.88\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 481.67 | loss  9.40 | perplexity 12052.37\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 480.63 | loss  9.40 | perplexity 12036.92\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 481.50 | loss  9.39 | perplexity 11967.65\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 482.19 | loss  9.39 | perplexity 11978.14\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 481.58 | loss  9.38 | perplexity 11828.07\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 483.43 | loss  9.39 | perplexity 11977.62\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 484.58 | loss  9.39 | perplexity 12007.28\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 481.72 | loss  9.39 | perplexity 12004.11\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 484.34 | loss  9.39 | perplexity 11992.41\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 504.50 | loss  9.39 | perplexity 12017.50\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 484.01 | loss  9.39 | perplexity 12019.70\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 482.39 | loss  9.39 | perplexity 12023.41\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 482.83 | loss  9.40 | perplexity 12040.52\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 180.48s| valid loss  9.39 | valid perplexity 11912.69\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12075.26\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 501.35 | loss  9.77 | perplexity 17529.88\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 482.42 | loss  9.40 | perplexity 12052.37\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 481.88 | loss  9.40 | perplexity 12036.92\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 484.15 | loss  9.39 | perplexity 11967.65\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 502.11 | loss  9.39 | perplexity 11978.14\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 485.24 | loss  9.38 | perplexity 11828.07\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 489.79 | loss  9.39 | perplexity 11977.62\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 482.07 | loss  9.39 | perplexity 12007.28\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 482.50 | loss  9.39 | perplexity 12004.11\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 482.93 | loss  9.39 | perplexity 11992.41\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 481.67 | loss  9.39 | perplexity 12017.50\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 482.44 | loss  9.39 | perplexity 12019.70\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 482.76 | loss  9.39 | perplexity 12023.41\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 482.35 | loss  9.40 | perplexity 12040.52\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 180.70s| valid loss  9.39 | valid perplexity 11912.69\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.40 | test perplexity 12075.26\n",
      "====================================================================================================\n",
      "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 501.80 | loss  9.76 | perplexity 17336.45\n",
      "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 481.58 | loss  9.38 | perplexity 11888.60\n",
      "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 484.83 | loss  9.39 | perplexity 11926.99\n",
      "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 482.52 | loss  9.38 | perplexity 11894.53\n",
      "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 482.32 | loss  9.38 | perplexity 11894.77\n",
      "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 483.40 | loss  9.39 | perplexity 11969.04\n",
      "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 482.72 | loss  9.39 | perplexity 11969.71\n",
      "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 483.61 | loss  9.38 | perplexity 11846.19\n",
      "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 487.60 | loss  9.39 | perplexity 11910.48\n",
      "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 486.48 | loss  9.38 | perplexity 11879.67\n",
      "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 481.41 | loss  9.39 | perplexity 11928.03\n",
      "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 481.89 | loss  9.39 | perplexity 11908.92\n",
      "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 483.33 | loss  9.38 | perplexity 11880.51\n",
      "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 483.40 | loss  9.38 | perplexity 11890.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 180.31s| valid loss  9.38 | valid perplexity 11868.79\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.38 | test perplexity 11896.92\n",
      "====================================================================================================\n",
      "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 507.75 | loss  9.76 | perplexity 17336.45\n",
      "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 483.48 | loss  9.38 | perplexity 11888.60\n",
      "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 486.55 | loss  9.39 | perplexity 11926.99\n",
      "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 501.92 | loss  9.38 | perplexity 11894.53\n",
      "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 483.25 | loss  9.38 | perplexity 11894.77\n",
      "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 494.75 | loss  9.39 | perplexity 11969.04\n",
      "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 489.02 | loss  9.39 | perplexity 11969.71\n",
      "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 482.68 | loss  9.38 | perplexity 11846.19\n",
      "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 486.59 | loss  9.39 | perplexity 11910.48\n",
      "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 483.20 | loss  9.38 | perplexity 11879.67\n",
      "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 494.64 | loss  9.39 | perplexity 11928.03\n",
      "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 486.18 | loss  9.39 | perplexity 11908.92\n",
      "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 483.60 | loss  9.38 | perplexity 11880.51\n",
      "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 492.00 | loss  9.38 | perplexity 11890.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 182.15s| valid loss  9.38 | valid perplexity 11868.79\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.38 | test perplexity 11896.92\n",
      "====================================================================================================\n",
      "| epoch   2 |    25/  363 batches | lr 5.00 | ms/batch 503.17 | loss  9.76 | perplexity 17336.45\n",
      "| epoch   2 |    50/  363 batches | lr 5.00 | ms/batch 484.93 | loss  9.38 | perplexity 11888.60\n",
      "| epoch   2 |    75/  363 batches | lr 5.00 | ms/batch 484.64 | loss  9.39 | perplexity 11926.99\n",
      "| epoch   2 |   100/  363 batches | lr 5.00 | ms/batch 484.56 | loss  9.38 | perplexity 11894.53\n",
      "| epoch   2 |   125/  363 batches | lr 5.00 | ms/batch 502.43 | loss  9.38 | perplexity 11894.77\n",
      "| epoch   2 |   150/  363 batches | lr 5.00 | ms/batch 484.24 | loss  9.39 | perplexity 11969.04\n",
      "| epoch   2 |   175/  363 batches | lr 5.00 | ms/batch 484.46 | loss  9.39 | perplexity 11969.71\n",
      "| epoch   2 |   200/  363 batches | lr 5.00 | ms/batch 487.38 | loss  9.38 | perplexity 11846.19\n",
      "| epoch   2 |   225/  363 batches | lr 5.00 | ms/batch 484.69 | loss  9.39 | perplexity 11910.48\n",
      "| epoch   2 |   250/  363 batches | lr 5.00 | ms/batch 485.46 | loss  9.38 | perplexity 11879.67\n",
      "| epoch   2 |   275/  363 batches | lr 5.00 | ms/batch 500.10 | loss  9.39 | perplexity 11928.03\n",
      "| epoch   2 |   300/  363 batches | lr 5.00 | ms/batch 485.37 | loss  9.39 | perplexity 11908.92\n",
      "| epoch   2 |   325/  363 batches | lr 5.00 | ms/batch 485.79 | loss  9.38 | perplexity 11880.51\n",
      "| epoch   2 |   350/  363 batches | lr 5.00 | ms/batch 499.07 | loss  9.38 | perplexity 11890.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 182.19s| valid loss  9.38 | valid perplexity 11868.79\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.38 | test perplexity 11896.92\n",
      "====================================================================================================\n",
      "| epoch   3 |    25/  363 batches | lr 1.25 | ms/batch 504.20 | loss  9.76 | perplexity 17336.45\n",
      "| epoch   3 |    50/  363 batches | lr 1.25 | ms/batch 485.54 | loss  9.38 | perplexity 11888.60\n",
      "| epoch   3 |    75/  363 batches | lr 1.25 | ms/batch 485.75 | loss  9.39 | perplexity 11926.99\n",
      "| epoch   3 |   100/  363 batches | lr 1.25 | ms/batch 509.61 | loss  9.38 | perplexity 11894.53\n",
      "| epoch   3 |   125/  363 batches | lr 1.25 | ms/batch 490.95 | loss  9.38 | perplexity 11894.77\n",
      "| epoch   3 |   150/  363 batches | lr 1.25 | ms/batch 492.89 | loss  9.39 | perplexity 11969.04\n",
      "| epoch   3 |   175/  363 batches | lr 1.25 | ms/batch 494.17 | loss  9.39 | perplexity 11969.71\n",
      "| epoch   3 |   200/  363 batches | lr 1.25 | ms/batch 485.03 | loss  9.38 | perplexity 11846.19\n",
      "| epoch   3 |   225/  363 batches | lr 1.25 | ms/batch 487.71 | loss  9.39 | perplexity 11910.48\n",
      "| epoch   3 |   250/  363 batches | lr 1.25 | ms/batch 489.66 | loss  9.38 | perplexity 11879.67\n",
      "| epoch   3 |   275/  363 batches | lr 1.25 | ms/batch 485.09 | loss  9.39 | perplexity 11928.03\n",
      "| epoch   3 |   300/  363 batches | lr 1.25 | ms/batch 484.56 | loss  9.39 | perplexity 11908.92\n",
      "| epoch   3 |   325/  363 batches | lr 1.25 | ms/batch 485.45 | loss  9.38 | perplexity 11880.51\n",
      "| epoch   3 |   350/  363 batches | lr 1.25 | ms/batch 492.61 | loss  9.38 | perplexity 11890.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 182.62s| valid loss  9.38 | valid perplexity 11868.79\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.38 | test perplexity 11896.92\n",
      "====================================================================================================\n",
      "| epoch   4 |    25/  363 batches | lr 0.31 | ms/batch 506.30 | loss  9.76 | perplexity 17336.45\n",
      "| epoch   4 |    50/  363 batches | lr 0.31 | ms/batch 560.44 | loss  9.38 | perplexity 11888.60\n",
      "| epoch   4 |    75/  363 batches | lr 0.31 | ms/batch 509.16 | loss  9.39 | perplexity 11926.99\n",
      "| epoch   4 |   100/  363 batches | lr 0.31 | ms/batch 485.03 | loss  9.38 | perplexity 11894.53\n",
      "| epoch   4 |   125/  363 batches | lr 0.31 | ms/batch 485.05 | loss  9.38 | perplexity 11894.77\n",
      "| epoch   4 |   150/  363 batches | lr 0.31 | ms/batch 483.79 | loss  9.39 | perplexity 11969.04\n",
      "| epoch   4 |   175/  363 batches | lr 0.31 | ms/batch 494.34 | loss  9.39 | perplexity 11969.71\n",
      "| epoch   4 |   200/  363 batches | lr 0.31 | ms/batch 547.41 | loss  9.38 | perplexity 11846.19\n",
      "| epoch   4 |   225/  363 batches | lr 0.31 | ms/batch 628.62 | loss  9.39 | perplexity 11910.48\n",
      "| epoch   4 |   250/  363 batches | lr 0.31 | ms/batch 520.29 | loss  9.38 | perplexity 11879.67\n",
      "| epoch   4 |   275/  363 batches | lr 0.31 | ms/batch 505.98 | loss  9.39 | perplexity 11928.03\n",
      "| epoch   4 |   300/  363 batches | lr 0.31 | ms/batch 508.15 | loss  9.39 | perplexity 11908.92\n",
      "| epoch   4 |   325/  363 batches | lr 0.31 | ms/batch 500.34 | loss  9.38 | perplexity 11880.51\n",
      "| epoch   4 |   350/  363 batches | lr 0.31 | ms/batch 517.88 | loss  9.38 | perplexity 11890.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 192.70s| valid loss  9.38 | valid perplexity 11868.79\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "| Test loss  9.38 | test perplexity 11896.92\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "loss_train = []\n",
    "\n",
    "CUT_AFTER_BATCHES = -1  # JUST FOR DEBUGGING: cut the loop after these number of batches. Set to -1 to ignore\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dictionary: u6.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, train_data_splits,\n",
    "          clipping: float, learning_rate: float, print_interval: int, epoch: int,\n",
    "          criterion: torch.nn.Module = torch.nn.NLLLoss()):\n",
    "    \"\"\"\n",
    "    Train the model. Training mode turned on to enable dropout.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (train_data_splits.size(0) - 1) // max_seq_len\n",
    "\n",
    "    loss_collector = []\n",
    "    \n",
    "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
    "        batch_data, batch_targets = u6.get_batch(train_data_splits, i, max_seq_len)\n",
    "        # ipdb.set_trace()\n",
    "        \n",
    "        # Don't forget it! Otherwise, the gradients are summed together!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Repackaging batches only keeps the value of start_hidden and disconnects its computational graph.\n",
    "        # If repackaging is not done the, gradients are calculated from the current point to the beginning\n",
    "        # of the sequence which becomes computationally too expensive.\n",
    "        if start_hidden is not None:\n",
    "            start_hidden = u6.repackage_hidden(start_hidden)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "        \n",
    "        # Loss computation & backward pass\n",
    "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "        loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # The last hidden states of the current step is set as the start hidden state of the next step.\n",
    "        # This passes the information of the current batch to the next batch.\n",
    "        start_hidden = last_hidden\n",
    "        \n",
    "        # Clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        \n",
    "        # Updating parameters using SGD\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_i % print_interval == 0 and batch_i > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            loss_collector.append(cur_loss)\n",
    "            elapsed = time.time() - start_time\n",
    "            throughput = elapsed * 1000 / print_interval\n",
    "            print(f\"| epoch {epoch:3d} | {batch_i:5d}/{n_batches:5d} batches | lr {learning_rate:02.2f} | ms/batch {throughput:5.2f} \"\n",
    "                  f\"| loss {cur_loss:5.2f} | perplexity {math.exp(cur_loss):8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    loss_train.append(loss_collector)\n",
    "            \n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "loss_val = []\n",
    "\n",
    "\n",
    "best_val_loss = None\n",
    "model = model1\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "model = model2\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "model = model3\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "epochs = 5  # total number of training epochs\n",
    "print_interval = 25  # print report statistics every x batches\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "model = model4\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "    loss_val.append(val_loss)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "          f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "    \n",
    "    with open(save_path, \"rb\") as f:\n",
    "        model = torch.load(f)\n",
    "        \n",
    "    test_loss = u6.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| Test loss {test_loss:5.2f} | test perplexity {math.exp(test_loss):5.2f}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x78ed85f52ef0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f52f20>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f53010>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f53100>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f531f0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f532e0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f533d0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f534c0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f535b0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f536a0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f53790>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f51ea0>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f53850>,\n",
       " <matplotlib.lines.Line2D at 0x78ed85f53940>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train, loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "Model 2 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "Model 3 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "Model 4 Shape: (L, B, 10001) (L=40, B=64)  \n",
    "  \n",
    "The plot turned out to be linear...  \n",
    "Since I assume this was not the intended result. It seems there was a problem with either my model implementations, or the transferring of code across notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Generation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4. [30 Points]</b>\n",
    "    <p>\n",
    "    Copy the language generation code from the main exercise notebook and perform the following tasks:\n",
    "    </p>\n",
    "        <ul>\n",
    "            <li>Compare all four previous models by generating $12$ words that append the starting word <tt>\"despite\"</tt>.</li>\n",
    "            <li>For each model, retrieve the top $10$ wordIDs with the highest probabilities from the generated probability distribution (<code>prob_dist</code>) following the starting word <tt>\"despite\"</tt>. Fetch the corresponding words of these wordIDs. Do you observe any specific linguistic characteristic common between these words?</li>\n",
    "            <li>The implementation in the main exercise notebook is based on sampling. Implement a second deterministic variant based on the <i>top-1</i> approach. In this particular variant, the generated word is the word with the highest probability in the predicted probability distribution. Repeat the same procedure as before (i.e., generate $12$ words that append the starting word <tt>\"despite\"</tt>).</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "['publishing', 'extensive', 'related', 'electronic', 'commonwealth', 'unsolicited', 'frozen', 'anticipates', 'accusations', 'crossing']\n",
      "['peat', 'encouraging', 'improvement', 'membership', 'trapped', 'racked', 'driving', 'uncle', 'improvement', 'specialized']\n",
      "['ambitious', 'compare', 'mrs.', 'maturing', 'marketer', 'lobbied', 'preventing', 'itt', 'bookings', 'dependents']\n",
      "['travelers', 'themes', 'dean', 'urges', 'covered', 'maryland', 'shelf', 'jenrette', 'girl', 'revealed']\n",
      "['buying', 'evenly', 'formed', 'disney', 'stock', 'austin', 'advertisements', 'paramount', 'shame', 'realty']\n",
      "['magna', 'heading', 'birthday', 'amgen', 'lin', 'arco', 'use', 'boiler', 'appropriations', 'gold']\n",
      "['pursued', 'distributor', 'institutional', 'nonperforming', 'loss', 'kohlberg', 'antibody', 'goals', 'voice', 'high-tech']\n",
      "['asked', 'films', 'cupertino', 'vivid', 'prices', 'kick', 'unspecified', 'beers', 'pie', 'larsen']\n",
      "[\"d'arcy\", 'challenging', 'regard', 'ethical', 'commerce', 'challenging', 'prolonged', 'hole', 'texaco', 'nickel']\n",
      "['implications', 'update', 'face-to-face', 'disagreement', 'depreciation', 'borough', 'pretoria', 'dress', 'lawsuits', 'mutual']\n",
      "['creative', 'neil', 'repaid', 'so', 'resignation', 'prospect', 'overseas', 'amgen', 'accelerating', 'artist']\n",
      "['retailing', 'seldom', 'cool', 'short-term', 'sponsor', 'disclosed', 'academy', 'score', 'schering-plough', 'double']\n",
      "despite cheap long-distance researchers welcomed disappearance discretion korea would-be insistence bureaucrat ratio fastest\n",
      "Model 1\n",
      "['withdrew', 'commute', 'esso', 'ackerman', 'eastman', 'skeptical', 'respectable', 'hung', 'expression', 'franchisee']\n",
      "['abolished', 'mesa', 'real-estate', 'addressing', 'robin', 'advertising', 'occur', 'backer', 'felt', 'lease']\n",
      "['jump', 'ticket', 'dire', 'j.', 'latest', 'norfolk', 'register', 'send', 'pools', 'industries']\n",
      "['trials', 'wears', 'hooker', 'pharmaceutical', 'men', 'border', 'captured', 'fraser', 'disarray', 'long']\n",
      "['entity', 'leftist', 'cooking', 'ratings', 'canton', 'weekly', 'canton', 'neglected', 'distributes', 'meet']\n",
      "['crossing', 'enfield', 'programs', 'summit', 'values', 'ray', 'candela', 'wedge', 'array', 'office']\n",
      "['comments', 'helped', 'fashionable', 'living', 'related', 'bottled', 'array', 'array', 'photographic', 'domination']\n",
      "['exists', 'mint', 'forgotten', 'grants', 'belongs', 'disappear', 'commodity', 'equitec', 'pressures', 'ufo']\n",
      "['scowcroft', 'inevitable', 'per', 'times', 'while', 'outsider', 'appropriated', 'chevy', 'extra', 'impeachment']\n",
      "['maybe', 'banned', 'parker', 'weaken', 'ohbayashi', 'ashland', 'complain', 'insider', 'weaken', 'executive']\n",
      "['few', 'casual', 'horse', 'whole', 'very', 'losing', 'few', 'wis.', 'fails', 'listings']\n",
      "['farmer', 'candidate', 'fix', 'four-year', 'stanza', 'spurring', 'treating', 'fires', 'weakening', 'discretion']\n",
      "despite stock wood just cairo improvement snack-food trimmed conversion believe dollar-denominated precious bowes\n",
      "Model 2\n",
      "['a.g.', 'krasnoyarsk', 'justify', 'upscale', 'fourth-quarter', 'tesoro', 'debris', 'pursuing', 'teach', 'alleging']\n",
      "['dell', 'carla', 'blocks', 'junk-bond', 'publishes', 'dealerships', 'compare', 'impossible', 'parks', 'sum']\n",
      "['shapiro', 'shanghai', 'mary', 'supplied', 'heard', 'liberals', 'buried', 'mechanical', 'white-collar', 'breakdown']\n",
      "['pentagon', 'hurt', 'friday', 'athletic', 'body', 'personally', 'revisions', 'sanctions', 'first-time', 'instituted']\n",
      "['resilience', 'anderson', 'donald', 'models', 'patients', 'portion', 'owns', 'legent', 'albany', 'animal']\n",
      "['genentech', 'rank', 'parcel', 'cents', 'salesman', 'fluctuations', 'steel', 'other', 'misstated', 'is']\n",
      "['money-fund', 'consistently', 'humans', 'rather', 'entertainment', 'earn', 'aspirations', 'alex', 'assist', 'my']\n",
      "['sidhpur', 'non-food', 'challenged', 'neil', 'briefing', 'n.h.', 'rate', 'intentionally', 'bit', 'aggregates']\n",
      "['divide', 'starts', 'attacking', 'argument', 'consent', 'policy', 'pleasure', 'stein', 'agnos', 'waste']\n",
      "['personal', 'supplying', 'casinos', 'leftist', 'leaders', 'damaged', 'correction', 'consent', 'disney', 'caught']\n",
      "['conceptual', 'estimate', 't.', 'saved', 'fall', 'deloitte', 'luxury-car', 'after', 'latest', 'successes']\n",
      "['athletic', 'automated', 'pickup', 'additional', 'microprocessor', 'upon', 'safer', 'lab', 'frankfurt', 'agencies']\n",
      "despite plaintiff asian freedoms italy christie decades atoms whose partially mental connected unveiled\n",
      "Model 3\n",
      "['shearson', 'finance', 'exchange', 'wind', 'create', 'additional', 'excessive', 'sigh', 'workers', 'plastic']\n",
      "['dollar', 'confederation', 'increased', 'wathen', 'attend', 'severe', 'changed', 'ind.', 'rhone-poulenc', 'redevelopment']\n",
      "['edelman', 'casual', 'write-down', 'editorial', 'banking', 'pit', 'challenges', 'short-term', 'swelling', 'screens']\n",
      "['conclude', 'small', 'jeff', 'morgenzon', 'introduced', 'voted', 'mood', 'numbers', 'emphasis', 'carbon']\n",
      "['ms.', 'before', 'mattel', 'help', 'elite', 'society', 'course', 'so-called', 'oliver', 'asked']\n",
      "['journal', 'let', 'parity', 'senate', 'banc', 'asset-backed', 'translation', 'so', 'biscuits', 'seasons']\n",
      "['battery', 'eventual', 'round', 'hear', 'r.h.', 'smoking', 'hertz', 'spouses', 'shevardnadze', 'ca']\n",
      "['myself', 'human-rights', 'awful', 'partners', 'burden', 'standard', 'fried', 'less', 'donoghue', 'lift']\n",
      "['heating', 'recognized', 'seeking', 'seniority', 'criticisms', 'highway', 'municipals', 'vacuum', 'crime', 'targeted']\n",
      "['diamond', 'greedy', 'construction', 'mentality', 'tangible', 'applicable', 'mountain', 'dillon', 'o.', 'deny']\n",
      "['branch', 'unpaid', 'public-relations', 'previously', 'appliances', 'desk', 'seems', 'prepaid', 'stone', 'commitment']\n",
      "['genetically', 'max', 'unfavorable', 'coopers', 'tree', 'loyal', 'alone', 'composed', 'enthusiasm', 'billings']\n",
      "despite nonperforming runkel arbitragers russian ferguson administrative probability dallas fully leather delaware breed\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "for model in model_list:\n",
    "    print(f\"Model {model_list.index(model)}\")\n",
    "    \n",
    "    GENERATION_LENGTH = 12\n",
    "    START_WORD = \"despite\"\n",
    "    \n",
    "    start_hidden = None\n",
    "    START_WORD = START_WORD.lower()\n",
    "    generated_text = START_WORD\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        wordid_input = dictionary.word2idx[START_WORD]\n",
    "        for i in range(0, GENERATION_LENGTH):\n",
    "            data = u6.batchify(torch.tensor([wordid_input]), 1, device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
    "            \n",
    "            prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze())\n",
    "            \n",
    "            samples = prob_dist.sample((1,10)).numpy()\n",
    "            sample_words = [dictionary.idx2word[i] for i in samples[0]]\n",
    "            print(sample_words)\n",
    "            \n",
    "            wordid_input = prob_dist.sample()\n",
    "            word_generated = dictionary.idx2word[wordid_input]\n",
    "            \n",
    "            generated_text += \" \" + word_generated\n",
    "            \n",
    "            start_hidden = last_hidden\n",
    "    \n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've noticed that most of the sampled words are either nouns, actions, or characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "despite guaranty items highly roles consumers hot-dipped slack conference prosecution gaubert loans whooping\n",
      "Model 1\n",
      "despite railway obligated submitted corporation differences photography tightening rank test brewing martin sooner\n",
      "Model 2\n",
      "despite bologna kids w. plummeted vancouver discussed security criteria suit semel agent computing\n",
      "Model 3\n",
      "despite chicken rider heating front induce publishers operating issuer ordered termination charges flowing\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    print(f\"Model {model_list.index(model)}\")\n",
    "    \n",
    "    GENERATION_LENGTH = 12\n",
    "    START_WORD = \"despite\"\n",
    "    \n",
    "    start_hidden = None\n",
    "    START_WORD = START_WORD.lower()\n",
    "        \n",
    "    generated_text = START_WORD\n",
    "    with torch.no_grad():\n",
    "        wordid_input = dictionary.word2idx[START_WORD]\n",
    "        for i in range(0, GENERATION_LENGTH):\n",
    "            data = u6.batchify(torch.tensor([wordid_input]), 1, device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
    "            \n",
    "            prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze()) \n",
    "            wordid_input = prob_dist.sample([1])\n",
    "            word_generated = dictionary.idx2word[wordid_input]\n",
    "            \n",
    "            generated_text += \" \" + word_generated\n",
    "            \n",
    "            start_hidden = last_hidden\n",
    "    \n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
